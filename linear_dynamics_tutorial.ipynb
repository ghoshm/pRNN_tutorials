{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd8558e",
   "metadata": {},
   "source": [
    "# Linear pRNN dynamics\n",
    "\n",
    "By [Marcus Ghosh](https://profiles.imperial.ac.uk/m.ghosh/)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ghoshm/pRNN_tutorials/blob/main/linear_dynamics_tutorial.ipynb)\n",
    "\n",
    "## Aim\n",
    "\n",
    "We're going to explore how a neural network's structure shapes its dynamics:   \n",
    "* **Structure**: how neurons are connected — the weight matrix. \n",
    "* **Dynamics**: how each neuron's activity (activation or output) evolves over time when you run the network.  \n",
    "\n",
    "There are 4 parts to this tutorial: \n",
    "* Understand the **models** - how can we build networks with different structures and define their dynamics. \n",
    "* **Simulate** model dynamics (numerically) and visualize their behaviour.  \n",
    "* **Solve** model dynamics (analytically). \n",
    "* **Extensions** - explore oscillatory dynamics, and more complex models.    \n",
    "\n",
    "Throughout instructions and questions are marked like this: \n",
    "\n",
    "> 0. Read, code or answer a question.\n",
    "\n",
    "When you need to fill in code, you will see a  ```pass``` statement. Replace this with your code! \n",
    "\n",
    "There are **10** instructions for you to tackle! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5553f2",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809606d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import colors\n",
    "import itertools\n",
    "import os \n",
    "\n",
    "# For Google Colab\n",
    "if not os.path.exists('src.py'):\n",
    "  !git clone https://github.com/ghoshm/pRNN_tutorials.git\n",
    "  %cd pRNN_tutorials\n",
    "\n",
    "from src import * # Import all functions from src.py\n",
    "\n",
    "plt.style.use(\"./style_sheet.mplstyle\")\n",
    "\n",
    "eps = 1e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ee0db1",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "To explore structure and dynamics, we need to create networks with different structures and then simulate or solve their dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406bbc5d",
   "metadata": {},
   "source": [
    "## Structure\n",
    "Instead of creating artificial neural networks with different architectures, we will build *toy* models.   \n",
    "\n",
    "> 1. Read this [short article](https://doi.org/10.53053/WUSL4267). Then discuss with your partner - what are toy models, what are their advantages compared to larger models, and what might we lose by focussing on them? \n",
    "\n",
    "We'll start by building models with:\n",
    "* Three neurons (nodes or units) - labelled input (*i*), hidden (*h*), output (*o*). While three neurons may seem small, you'll see that this size allows us to explore a complete set of structures, with surprisingly diverse dynamics.  \n",
    "* A <span style='color: #59656d;'>feedforward</span> connection from the input to the hidden neuron (*ih*). \n",
    "* A <span style='color: #59656d;'>feedforward</span> connection from the hidden to the output neuron (*ho*). \n",
    "\n",
    "This is a *feedforward* structure - as signals can only travel from the input to the output neuron.\n",
    "\n",
    "But, there are $7$ other connections we could add, each of which will change the models structure and its signal flow: \n",
    "* <span style='color: #0189a0;'>Lateral (or recurrent)</span> connections from each neuron to itself: *ii*, *hh*, *oo*. \n",
    "* <span style='color: #13bbaf;'> Skip</span> connections which bypass the hidden neuron: *io*, *oi*. \n",
    "* <span style='color: #f97306;'> Backwards</span> connections: *hi*, *oh*. \n",
    "\n",
    "This figure shows our model with its: \n",
    "* $3$ nodes.\n",
    "* $2$ feed-forward connections - in grey.\n",
    "* All $7$ of the other possible connections: lateral - blue, skip - green, backwards - orange.\n",
    "\n",
    "<img src=\"./images/FR_pathways_labs.png\" width=\"400\" >\n",
    "\n",
    "By keeping the $2$ feed-forward connections (so that signals can always travel from the input to the output), and allowing each of the other connections to be present or absent in any combination ($7$ binary choices), we can generate:\n",
    "\n",
    "$2^{(3+2+2)} = 2^7 = 128$ unique network structures \n",
    "\n",
    "In [Ghosh & Goodman, 2025](https://doi.org/10.1101/2025.07.28.667142) we term these *partially recurrent neural networks* (pRNNs). \n",
    "\n",
    "Running the code below will:\n",
    "* Create a list with an adjacency matrix per structure. These are binary $(3,3)$ matrices with connections $(1)$ between (source, target). So W[src, tgt] = 1 means: an connection from neuron src → neuron tgt.\n",
    " \n",
    "* Plot all $128$ pRNN structures - with their $3$ neurons (circles) and connections (arrows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all pRNN structures \n",
    "\n",
    "wm_flags = np.array(list(itertools.product([0,1], repeat=7)), dtype=int) # array: (architectures, extra connection pathways)\n",
    "\n",
    "adj_ms = [create_pRNN_adj_matrix(wm_flag=f) for f in wm_flags] # list: architectures (source, target).             \n",
    "\n",
    "# Plot \n",
    "fig, ax = plt.subplots(nrows=8, ncols=16, figsize=(30,15), sharex=True, sharey=True)\n",
    "for a, _ in enumerate(ax.ravel()):\n",
    "    plt.sca(ax.ravel()[a])\n",
    "    plot_pRNN_architecture(wm_flags[a], ax=ax.ravel()[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb06ebd",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "\n",
    "Instead of allowing each neuron to compute a non-linear input-output transformation (such as ReLU - in machine learning or leaky integrate-and-fire in computational neuroscience) we will treat our models as *linear dynamical systems*. \n",
    "\n",
    "At every time-step ($t$) a network's state is defined by a vector ($x_t$) with each node's activation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_t = [i_t, h_t, o_t]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To determine how this state changes over time, we define an update:\n",
    "\n",
    "$$ \n",
    "x_{t+1} = x_tW\n",
    "$$ \n",
    "\n",
    "Where $W$ is the network's ($3,3$) adjacency matrix.\n",
    "\n",
    "Even though these models are linear, activity may grow, persist, decay or oscillate depending on the structure! \n",
    "\n",
    "> 2. Calculate $x_1$, $x_2$ and $x_3$ by hand, after starting the purely feed-forward model (which only has *ih* and *ho* connections) from the state $x_0 = [1,0,0]$ - which is like inputting a signal to the network. How does the network's state evolve over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42065706",
   "metadata": {},
   "source": [
    "# Simulations\n",
    "\n",
    "One approach to exploring each structure's dynamics is to use numerical simulations. \n",
    "\n",
    "To do so, we just need to: \n",
    "* Set an initial state ($x_0$).  \n",
    "* Run the state update (above) for a fixed number of time steps ($T$).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_pRNN_dynamics(adj_ms, x0, T): \n",
    "    \"\"\"\n",
    "    Simulate dynamics from an initial state. \n",
    "    Arguments: \n",
    "        adj_ms: list of weight matrices (source, target).\n",
    "        x0: an initial network state, array: (nodes).   \n",
    "        T: the number of steps to simulate for, int. \n",
    "    Returns: \n",
    "        x_hist: an array of dynamics, (architectures, time, nodes). \n",
    "    \"\"\"\n",
    "    x_hist = np.zeros((len(adj_ms), T + 1, len(x0)), dtype=float) # array: (architectures, time, nodes)\n",
    "\n",
    "    for a, W in enumerate(adj_ms):\n",
    "        x = x0.copy() # current state\n",
    "        x_hist[a, 0] = x # store\n",
    "\n",
    "        for t in range(T):\n",
    "            x = x @ W # update current state\n",
    "            x_hist[a, t + 1] = x # store\n",
    "\n",
    "    return x_hist\n",
    "\n",
    "x0 = np.array([1.0, 0.0, 0.0]) # initial state to simulate from (input, hidden, output)\n",
    "T = 15 # number of time steps to simulate\n",
    "\n",
    "x_hist = simulate_pRNN_dynamics(adj_ms, x0, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed383f0",
   "metadata": {},
   "source": [
    "> 3. Check that the activity in the feed-forward model `x_hist[0]` matches your answer above. If not, what did you get wrong?\n",
    "\n",
    "Now let's plot each model's dynamics. The function below will plot $128$ subplots ($1$ per model). Within each subplot:\n",
    "* The x-axis is time.\n",
    "* The y-axis is activity. \n",
    "* There are $3$ lines - representing the activity of the <span style='color: #d8dcd6;'> input</span> (grey), <span style='color: #7e1e9c;'> hidden</span> (purple) and <span style='color: #1ef876;'> output</span> (green) neuron over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure \n",
    "plot_pRNN_dynamics(x_hist=x_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642074ba",
   "metadata": {},
   "source": [
    "In the figure above it is hard to compare dynamics across architectures! \n",
    "\n",
    "One reason for this is that in different models, neurons can receive very different amounts of input (depending on their number of connections).\n",
    "\n",
    "For example, in the least complex model (`adj_ms[0]`) the output neuron receives one input. While, in the most complex model (`adj_ms[-1]`) the output unit receives three inputs! \n",
    "\n",
    "To rectify this, we can normalise the adjacency matrices in `adj_ms`. \n",
    "\n",
    "> 4. In the cell below, normalise each adjacency matrix so that each neuron receives either no input ($0$) or a total input of $1$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9828858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise \n",
    "for a, W in enumerate(adj_ms):\n",
    "    pass # Fill in your code here!\n",
    "\n",
    "# Simulate\n",
    "x_hist = simulate_pRNN_dynamics(adj_ms, x0, T)\n",
    "\n",
    "# Figure \n",
    "plot_pRNN_dynamics(x_hist=x_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae36767",
   "metadata": {},
   "source": [
    "If you have done this correctly, you will now see very different dynamics across the different models!\n",
    "\n",
    "> 5. In the cell below use calculate the number of structures with unique dynamics (from  ```x_hist```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass # Fill in your code here! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e35bcc",
   "metadata": {},
   "source": [
    "# Solvable\n",
    "\n",
    "Another way to understand each pRNNs behaviour is to look at it [eigendecomposition](https://www.datacamp.com/tutorial/eigendecomposition). \n",
    "\n",
    "This method factors (decomposes) a square matrix ($A$) into three matrices. \n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "A = PDP^{-1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $P$ - contains eigenvectors (the characteristic \"modes\" of the system). \n",
    "* $D$ - contains eigenvalues ($\\lambda$) on the diagonal. \n",
    "* $P^{-1}$ - the inverse of $P$.  \n",
    "\n",
    "For our networks, the eigenvectors correspond to activity patterns that evolve over time, and the eigenvalues tell us how each pattern changes:\n",
    "\n",
    "* $|\\lambda|$ > 1: grows.\n",
    "* $|\\lambda|$ ~ 1: persists.\n",
    "* $|\\lambda|$ < 1: decays.\n",
    "* complex $\\lambda$: oscillations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422830ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigendecomposition \n",
    "\n",
    "eigvals_list = []\n",
    "eigvecs_list = []\n",
    "for W in adj_ms:\n",
    "    vals, vecs = np.linalg.eig(W.T) # transpose as we use a row-vector update.\n",
    "    eigvals_list.append(vals)\n",
    "    eigvecs_list.append(vecs)\n",
    "\n",
    "eigvals = np.array(eigvals_list) # array: (128, 3)\n",
    "eigvecs = np.array(eigvecs_list) # array: (128, 3, 3)\n",
    "# Note: eigvecs[a, :, b] is the eigenvector corresponding to eigvals[a, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435aa63",
   "metadata": {},
   "source": [
    "The long‑term behaviour of each network is dictated by its spectral radius: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\rho (W) = \\max_{i} |\\lambda_i|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> 6. In the cell below calculate each structure's spectral radius.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "\n",
    "gs = [] # a list to store each architectures spectral radius\n",
    "\n",
    "for a in range(len(eigvals)):\n",
    "    pass # Fill in code here\n",
    "\n",
    "gs = np.array(gs) # array: 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad521e4",
   "metadata": {},
   "source": [
    "The code below will plot the spectral radius (y-axis) of each architecture (x-axis) sorted from lowest to highest. \n",
    "\n",
    "And will highlight the <span style='color: #5a86ad;'> feedforward model</span> (in blue), the <span style='color: #1fb57a;'> RNN model</span> (in green) and the <span style='color: #7e1e9c;'> fully recurrent model </span> (in purple). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daaa3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure  \n",
    "\n",
    "interest = [0, 32, 127]\n",
    "i_cols = ['xkcd:dusty blue', 'xkcd:dark seafoam', 'xkcd:purple']\n",
    "i_labels = ['Feedforward', 'RNN', 'Fully recurrent']\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.scatter(range(len(gs)), np.sort(gs), s=2, c='k')\n",
    "\n",
    "for a, i in enumerate(interest): \n",
    "    ms, sl, bl = plt.stem(np.where(np.argsort(gs) == i), gs[i])\n",
    "    ms.set_markersize(2)\n",
    "    ms.set_markerfacecolor(i_cols[a])\n",
    "    ms.set_markeredgecolor(i_cols[a])\n",
    "    sl.set_linewidth(2)\n",
    "    sl.set_edgecolor(i_cols[a])\n",
    "\n",
    "plt.xticks([])\n",
    "plt.xlabel(\"Architectures\")\n",
    "plt.ylabel(r\"$\\rho(W)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ee5f2",
   "metadata": {},
   "source": [
    "Assuming everything is correct, you should see that for most architectures $\\rho (W) = 1$, but for some $\\rho (W) < 1$. If not, you may need to check your normalisation or how you calculate $\\rho (W)$.    \n",
    "\n",
    "The figure below will plot every *pRNN* structure. Highlighting those with $\\rho (W) < 1$ in green. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce485d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure \n",
    "fig, ax = plt.subplots(\n",
    "    nrows=8, ncols=16, figsize=(30, 15),\n",
    "    sharex=True, sharey=True\n",
    ")\n",
    "\n",
    "for a, _ in enumerate(ax.ravel()):\n",
    "    plt.sca(ax.ravel()[a])\n",
    "    if gs[a] < 0.99:\n",
    "        plot_pRNN_architecture(wm_flag=wm_flags[a], ax=ax.ravel()[a], color=\"xkcd:dark seafoam\")\n",
    "    else: \n",
    "        plot_pRNN_architecture(wm_flag=wm_flags[a], ax=ax.ravel()[a], color=\"xkcd:light grey\")\n",
    "\n",
    "    if wm_flags[a][1] == 1: # A first guess at a structural rule \n",
    "        plt.scatter(1,1, color='xkcd:light purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4e35c",
   "metadata": {},
   "source": [
    "> 7. What feature or features do all of the structures with $\\rho (W) < 1$ share? Can you write a *structural rule* - which identifies these? \n",
    "\n",
    "The code above tries the rule: structures with *hh* connections, and adds a purple dot to those structures. By comparing the green structures (those with $\\rho (W) < 1$) and those with a purple dot (with *hh* connections), we can see that this rule isn't very predictive. Try to discover the correct rule!   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd65233",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "\n",
    "Now you've got the hang of thinking about structure and dynamics, try working on these extensions (in any order):\n",
    "\n",
    "8. **Oscillations** - some pRNNs will show oscillatory dynamics. You can find these by looking at your simulation results or by finding structures with complex eigenvalues. Can you find a *structural rule* (as above) which predicts which structures will oscillate and which will not? \n",
    "\n",
    "9. **Wider structures** - by adding an extra input neuron (*i0*, *i1*, *h*, *o*), keeping a similar feed-forward structure (*i0->h*, *i1->h*, *h->o*) and allowing all of the other possible connections to be present or absent in different combinations you could generate a larger space of $8,192$ structures. Generate these models and explore their dynamics. \n",
    "\n",
    "10. **Deeper structures** - by adding an extra hidden neuron (*i*, *h0*, *h1*, *o*), keeping a similar feed-forward structure (*i->h0*, *h0->h1*, *h1->o*) and allowing all of the other possible connections to be present or absent in different combinations you could generate a larger space of $8,192$ structures. Generate these models and explore their dynamics.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pRNN_tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
